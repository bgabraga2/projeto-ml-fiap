{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Modelo de Classificação: Predição de Recompra em 30 dias\n",
    "\n",
    "Este notebook implementa um modelo de machine learning para prever a probabilidade de recompra de clientes em um horizonte de 30 dias, utilizando dados históricos de transações de uma empresa de transporte rodoviário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Bibliotecas de machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Utilitários\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "data_loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso!\n",
      "Shape do dataset: (1741344, 12)\n",
      "\n",
      "==================================================\n",
      "PRIMEIRAS 5 LINHAS:\n",
      "                                 nk_ota_localizer_id  \\\n",
      "0  bc02d5245bec63b30ff1102fa273fc03f58bc9cc3f674e...   \n",
      "1  5432f12612dd5d749b3be880e779989cf63b5efa4bcc4e...   \n",
      "2  fb3caed9b2f1b6016d45ccddb19095476e61a2c85faa8e...   \n",
      "3  4dc44a6dd592b702feccb493d192210c86965aee684529...   \n",
      "4  aa34ed7fd0a6b405df2df1bf9f8d68e6df9b9a868a6181...   \n",
      "\n",
      "                                          fk_contact date_purchase  \\\n",
      "0  a7218ff4ee7d37d48d2b4391b955627cb089870b934912...    2018-12-26   \n",
      "1  37228485e0dc83d84d1bcd1bef3dc632301bf6cb22c8b5...    2018-12-05   \n",
      "2  3467ec081e2421e72c96e7203b929d21927fd00b6b5f28...    2018-12-21   \n",
      "3  ab3251a2be0f69713b8f97b0e9d1579e31551f4fd4facf...    2018-12-06   \n",
      "4  ceea0de820a6379f2c4215bddaec66c33994b304607e56...    2021-02-23   \n",
      "\n",
      "  time_purchase                             place_origin_departure  \\\n",
      "0      15:33:35  6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...   \n",
      "1      15:07:57  10e4e7caf8b078429bb1c80b1a10118ac6f963eff098fd...   \n",
      "2      18:41:54  7688b6ef52555962d008fff894223582c484517cea7da4...   \n",
      "3      14:01:38  4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...   \n",
      "4      20:08:25  7688b6ef52555962d008fff894223582c484517cea7da4...   \n",
      "\n",
      "                         place_destination_departure place_origin_return  \\\n",
      "0  50e9a8665b62c8d68bccc77c7c92431a1aa26ccbd38ed4...                   0   \n",
      "1  e6d41d208672a4e50b86d959f4a6254975e6fb9b088116...                   0   \n",
      "2  8c1f1046219ddd216a023f792356ddf127fce372a72ec9...                   0   \n",
      "3  d6acb3c1a79e57bcc03d976cb4d98f56edccd4cf426392...                   0   \n",
      "4  23765fc69c4e3c0b10f5d15471dc2245e2a19af16b513f...                   0   \n",
      "\n",
      "  place_destination_return                       fk_departure_ota_bus_company  \\\n",
      "0                        0  8527a891e224136950ff32ca212b45bc93f69fbb801c3b...   \n",
      "1                        0  36ebe205bcdfc499a25e6923f4450fa8d48196ceb4fa0c...   \n",
      "2                        0  ec2e990b934dde55cb87300629cedfc21b15cd28bbcf77...   \n",
      "3                        0  5f9c4ab08cac7457e9111a30e4664920607ea2c115a143...   \n",
      "4                        0  48449a14a4ff7d79bb7a1b6f3d488eba397c36ef25634c...   \n",
      "\n",
      "  fk_return_ota_bus_company  gmv_success  total_tickets_quantity_success  \n",
      "0                         1        89.09                               1  \n",
      "1                         1       155.97                               1  \n",
      "2                         1       121.99                               1  \n",
      "3                         1        55.22                               1  \n",
      "4                         1        45.31                               1  \n",
      "\n",
      "==================================================\n",
      "INFORMAÇÕES GERAIS:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1741344 entries, 0 to 1741343\n",
      "Data columns (total 12 columns):\n",
      " #   Column                          Dtype  \n",
      "---  ------                          -----  \n",
      " 0   nk_ota_localizer_id             object \n",
      " 1   fk_contact                      object \n",
      " 2   date_purchase                   object \n",
      " 3   time_purchase                   object \n",
      " 4   place_origin_departure          object \n",
      " 5   place_destination_departure     object \n",
      " 6   place_origin_return             object \n",
      " 7   place_destination_return        object \n",
      " 8   fk_departure_ota_bus_company    object \n",
      " 9   fk_return_ota_bus_company       object \n",
      " 10  gmv_success                     float64\n",
      " 11  total_tickets_quantity_success  int64  \n",
      "dtypes: float64(1), int64(1), object(10)\n",
      "memory usage: 159.4+ MB\n",
      "None\n",
      "\n",
      "==================================================\n",
      "VALORES NULOS:\n",
      "nk_ota_localizer_id               0\n",
      "fk_contact                        0\n",
      "date_purchase                     0\n",
      "time_purchase                     0\n",
      "place_origin_departure            0\n",
      "place_destination_departure       0\n",
      "place_origin_return               0\n",
      "place_destination_return          0\n",
      "fk_departure_ota_bus_company      0\n",
      "fk_return_ota_bus_company         0\n",
      "gmv_success                       0\n",
      "total_tickets_quantity_success    0\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "ESTATÍSTICAS DESCRITIVAS:\n",
      "        gmv_success  total_tickets_quantity_success\n",
      "count  1.741344e+06                    1.741344e+06\n",
      "mean   1.610925e+02                    1.394209e+00\n",
      "std    1.666552e+02                    8.096860e-01\n",
      "min   -3.656570e+03                    1.000000e+00\n",
      "25%    6.153000e+01                    1.000000e+00\n",
      "50%    1.141800e+02                    1.000000e+00\n",
      "75%    1.981800e+02                    2.000000e+00\n",
      "max    6.723930e+03                    1.600000e+01\n",
      "\n",
      "==================================================\n",
      "AMOSTRA DE DATAS:\n",
      "0    2018-12-26\n",
      "1    2018-12-05\n",
      "2    2018-12-21\n",
      "3    2018-12-06\n",
      "4    2021-02-23\n",
      "5    2021-02-11\n",
      "6    2021-02-19\n",
      "7    2020-12-03\n",
      "8    2020-12-04\n",
      "9    2021-07-02\n",
      "Name: date_purchase, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Carregamento e análise exploratória dos dados\n",
    "df = pd.read_csv('files/df_t.csv')\n",
    "\n",
    "print(\"Dados carregados com sucesso!\")\n",
    "print(f\"Shape do dataset: {df.shape}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PRIMEIRAS 5 LINHAS:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INFORMAÇÕES GERAIS:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALORES NULOS:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ESTATÍSTICAS DESCRITIVAS:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AMOSTRA DE DATAS:\")\n",
    "print(df['date_purchase'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "temporal_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANÁLISE TEMPORAL:\n",
      "Data mais antiga: 2013-09-12 00:00:00\n",
      "Data mais recente: 2024-04-01 00:00:00\n",
      "Período total: 3854 dias\n",
      "\n",
      "==================================================\n",
      "ANÁLISE DE CLIENTES:\n",
      "Total de clientes únicos: 581,817\n",
      "Média de compras por cliente: 2.99\n",
      "\n",
      "==================================================\n",
      "DISTRIBUIÇÃO POR ANO:\n",
      "year\n",
      "2013       341\n",
      "2014     15182\n",
      "2015     29654\n",
      "2016     54622\n",
      "2017     81354\n",
      "2018    100277\n",
      "2019    180935\n",
      "2020    145669\n",
      "2021    262649\n",
      "2022    380073\n",
      "2023    383163\n",
      "2024    107425\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "ANÁLISE DE VALORES:\n",
      "Compras com GMV positivo: 1,741,159\n",
      "Compras com GMV negativo: 7\n",
      "Compras com GMV zero: 178\n",
      "\n",
      "Data de corte para criar target: 2024-03-02 00:00:00\n",
      "Registros após data de corte: 34,185\n"
     ]
    }
   ],
   "source": [
    "# Análise temporal dos dados\n",
    "df['date_purchase'] = pd.to_datetime(df['date_purchase'])\n",
    "\n",
    "print(\"ANÁLISE TEMPORAL:\")\n",
    "print(f\"Data mais antiga: {df['date_purchase'].min()}\")\n",
    "print(f\"Data mais recente: {df['date_purchase'].max()}\")\n",
    "print(f\"Período total: {(df['date_purchase'].max() - df['date_purchase'].min()).days} dias\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANÁLISE DE CLIENTES:\")\n",
    "total_clientes = df['fk_contact'].nunique()\n",
    "print(f\"Total de clientes únicos: {total_clientes:,}\")\n",
    "print(f\"Média de compras por cliente: {len(df) / total_clientes:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DISTRIBUIÇÃO POR ANO:\")\n",
    "df['year'] = df['date_purchase'].dt.year\n",
    "print(df['year'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANÁLISE DE VALORES:\")\n",
    "print(f\"Compras com GMV positivo: {(df['gmv_success'] > 0).sum():,}\")\n",
    "print(f\"Compras com GMV negativo: {(df['gmv_success'] < 0).sum():,}\")\n",
    "print(f\"Compras com GMV zero: {(df['gmv_success'] == 0).sum():,}\")\n",
    "\n",
    "# Definir data de corte\n",
    "data_corte = df['date_purchase'].max() - timedelta(days=30)\n",
    "print(f\"\\nData de corte para criar target: {data_corte}\")\n",
    "print(f\"Registros após data de corte: {(df['date_purchase'] > data_corte).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dataset_preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data de corte: 2024-03-01\n",
      "Janela de observação: até 2024-03-31\n",
      "Criando dataset para modelagem...\n",
      "Compras históricas: 1,705,101\n",
      "Clientes únicos: 572,993\n",
      "Clientes que compraram na janela futura: 23,768\n",
      "\n",
      "Distribuição do Target:\n",
      "Não compraram (0): 557,768 (97.3%)\n",
      "Compraram (1): 15,225 (2.7%)\n",
      "\n",
      "Dataset base: (572993, 8)\n",
      "                                          fk_contact data_ultima_compra  \\\n",
      "0  0000029b76ad3cf9d86ad430754fb1d4478069affda61e...         2021-01-09   \n",
      "1  000010ae2e13049769982d9f07de792d92452ff1d124e3...         2022-05-05   \n",
      "2  00001f68902d3e8d332baa62a69065ce71e7b5a8c850a5...         2018-11-01   \n",
      "3  00007a5d618cd250d7f05766cfe01a8663a3767f1cd669...         2022-12-04   \n",
      "4  00008c39885815e42a0bb750cee199cd4da741a5645705...         2021-10-01   \n",
      "\n",
      "   gmv_ultima_compra  tickets_ultima_compra  \\\n",
      "0              91.02                      1   \n",
      "1              79.52                      1   \n",
      "2             169.90                      1   \n",
      "3             131.49                      1   \n",
      "4              58.88                      1   \n",
      "\n",
      "                                       origem_ultima  \\\n",
      "0  2fca346db656187102ce806ac732e06a62df0dbb2829e5...   \n",
      "1  f369cb89fc627e668987007d121ed1eacdc01db9e28f8b...   \n",
      "2  6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...   \n",
      "3  7688b6ef52555962d008fff894223582c484517cea7da4...   \n",
      "4  7688b6ef52555962d008fff894223582c484517cea7da4...   \n",
      "\n",
      "                                      destino_ultima  \\\n",
      "0  6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...   \n",
      "1  5514a9f709310b22ee9bddd4e6da1b2b8b04d1ad5c3dcb...   \n",
      "2  fc72c98a6c2916c1bbf9f39fce094f5785bb6f1d656971...   \n",
      "3  81b8a03f97e8787c53fe1a86bda042b6f0de9b0ec9c093...   \n",
      "4  4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...   \n",
      "\n",
      "                                      empresa_ultima  target  \n",
      "0  620c9c332101a5bae955c66ae72268fbcd397276617952...       0  \n",
      "1  811786ad1ae74adfdd20dd0372abaaebc6246e343aebd0...       0  \n",
      "2  9f14025af0065b30e47e23ebb3b491d39ae8ed17d33739...       0  \n",
      "3  35135aaa6cc23891b40cb3f378c53a17a1127210ce60e1...       0  \n",
      "4  ec2e990b934dde55cb87300629cedfc21b15cd28bbcf77...       0  \n",
      "\n",
      "Dataset base criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Preparação do dataset para modelagem preditiva\n",
    "\n",
    "# Definir data de corte para separar dados históricos dos dados de validação\n",
    "data_corte = pd.to_datetime('2024-03-01')\n",
    "data_fim_observacao = data_corte + timedelta(days=30)\n",
    "\n",
    "print(f\"Data de corte: {data_corte.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Janela de observação: até {data_fim_observacao.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"Criando dataset para modelagem...\")\n",
    "\n",
    "# 1. Separar dados históricos (antes do corte)\n",
    "dados_historicos = df[df['date_purchase'] < data_corte].copy()\n",
    "print(f\"Compras históricas: {len(dados_historicos):,}\")\n",
    "\n",
    "# 2. Última compra de cada cliente no período histórico\n",
    "ultima_compra = dados_historicos.groupby('fk_contact').agg({\n",
    "    'date_purchase': 'max',\n",
    "    'gmv_success': 'last',\n",
    "    'total_tickets_quantity_success': 'last',\n",
    "    'place_origin_departure': 'last',\n",
    "    'place_destination_departure': 'last',\n",
    "    'fk_departure_ota_bus_company': 'last'\n",
    "}).reset_index()\n",
    "\n",
    "ultima_compra.columns = ['fk_contact', 'data_ultima_compra', 'gmv_ultima_compra', \n",
    "                        'tickets_ultima_compra', 'origem_ultima', 'destino_ultima', 'empresa_ultima']\n",
    "\n",
    "print(f\"Clientes únicos: {len(ultima_compra):,}\")\n",
    "\n",
    "# 3. Identificar quem comprou na janela futura para criar target\n",
    "compras_futuras = df[\n",
    "    (df['date_purchase'] >= data_corte) & \n",
    "    (df['date_purchase'] <= data_fim_observacao)\n",
    "]['fk_contact'].unique()\n",
    "\n",
    "print(f\"Clientes que compraram na janela futura: {len(compras_futuras):,}\")\n",
    "\n",
    "# 4. Criar variável target\n",
    "ultima_compra['target'] = ultima_compra['fk_contact'].isin(compras_futuras).astype(int)\n",
    "\n",
    "# Verificar distribuição do target\n",
    "target_dist = ultima_compra['target'].value_counts()\n",
    "print(f\"\\nDistribuição do Target:\")\n",
    "print(f\"Não compraram (0): {target_dist[0]:,} ({target_dist[0]/len(ultima_compra)*100:.1f}%)\")\n",
    "print(f\"Compraram (1): {target_dist[1]:,} ({target_dist[1]/len(ultima_compra)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDataset base: {ultima_compra.shape}\")\n",
    "print(ultima_compra.head())\n",
    "\n",
    "dataset_base = ultima_compra.copy()\n",
    "print(\"\\nDataset base criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando criação de features comportamentais...\n",
      "Utilizando 1,705,101 compras históricas\n"
     ]
    }
   ],
   "source": [
    "# Engenharia de Features para Modelagem Preditiva\n",
    "\n",
    "print(\"Iniciando criação de features comportamentais...\")\n",
    "print(f\"Utilizando {len(dados_historicos):,} compras históricas\")\n",
    "\n",
    "# 1. Features de Recência\n",
    "dataset_base['dias_desde_ultima_compra'] = (data_corte - dataset_base['data_ultima_compra']).dt.days\n",
    "\n",
    "# 2. Features de Frequência\n",
    "freq_features = dados_historicos.groupby('fk_contact').agg({\n",
    "    'date_purchase': ['count', 'nunique'],\n",
    "    'gmv_success': ['sum', 'mean', 'std', 'min', 'max'],\n",
    "    'total_tickets_quantity_success': ['sum', 'mean', 'max']\n",
    "}).round(2)\n",
    "\n",
    "freq_features.columns = [\n",
    "    'total_compras', 'dias_unicos_compra',\n",
    "    'gmv_total', 'gmv_medio', 'gmv_std', 'gmv_min', 'gmv_max',\n",
    "    'tickets_total', 'tickets_medio', 'tickets_max'\n",
    "]\n",
    "freq_features = freq_features.fillna(0)\n",
    "\n",
    "# 3. Features Temporais e Sazonalidade\n",
    "dados_historicos['mes'] = dados_historicos['date_purchase'].dt.month\n",
    "dados_historicos['dia_semana'] = dados_historicos['date_purchase'].dt.dayofweek\n",
    "dados_historicos['hora'] = dados_historicos['time_purchase'].str[:2].astype(int)\n",
    "\n",
    "sazon_features = dados_historicos.groupby('fk_contact').agg({\n",
    "    'mes': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "    'dia_semana': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "    'hora': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "sazon_features.columns = ['mes_preferido', 'dia_semana_preferido', 'hora_media', 'hora_std']\n",
    "sazon_features = sazon_features.fillna(0)\n",
    "\n",
    "# 4. Features de Comportamento de Viagem\n",
    "comportamento_features = dados_historicos.groupby('fk_contact').agg({\n",
    "    'place_origin_departure': 'nunique',\n",
    "    'place_destination_departure': 'nunique',\n",
    "    'fk_departure_ota_bus_company': 'nunique'\n",
    "})\n",
    "\n",
    "comportamento_features.columns = ['origens_unicas', 'destinos_unicos', 'empresas_unicas']\n",
    "\n",
    "# 5. Features de Periodicidade\n",
    "intervalos = dados_historicos.groupby('fk_contact')['date_purchase'].apply(\n",
    "    lambda x: x.sort_values().diff().dt.days.mean() if len(x) > 1 else 0\n",
    ").round(2)\n",
    "\n",
    "periodicidade_features = pd.DataFrame({\n",
    "    'intervalo_medio_dias': intervalos,\n",
    "    'regularidade': dados_historicos.groupby('fk_contact')['date_purchase'].apply(\n",
    "        lambda x: x.sort_values().diff().dt.days.std() if len(x) > 1 else 0\n",
    "    ).round(2)\n",
    "}).fillna(0)\n",
    "\n",
    "# 6. Consolidação das Features\n",
    "dataset_final = dataset_base.copy()\n",
    "\n",
    "for features_df in [freq_features, sazon_features, comportamento_features, periodicidade_features]:\n",
    "    dataset_final = dataset_final.merge(features_df, left_on='fk_contact', right_index=True, how='left')\n",
    "\n",
    "dataset_final = dataset_final.fillna(0)\n",
    "\n",
    "print(f\"\\nDataset final criado!\")\n",
    "print(f\"Shape: {dataset_final.shape}\")\n",
    "print(f\"Total de features: {dataset_final.shape[1] - 8}\")\n",
    "\n",
    "feature_cols = [col for col in dataset_final.columns if col not in ['fk_contact', 'data_ultima_compra', 'target']]\n",
    "print(f\"\\nEstatísticas das principais features:\")\n",
    "print(dataset_final[feature_cols[:10]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação dos dados para modelagem\n",
    "\n",
    "print(\"Preparando dados para treinamento...\")\n",
    "\n",
    "# Separar features e target\n",
    "feature_cols = [col for col in dataset_final.columns if col not in ['fk_contact', 'data_ultima_compra', 'target']]\n",
    "X = dataset_final[feature_cols].copy()\n",
    "y = dataset_final['target'].copy()\n",
    "\n",
    "print(f\"Features selecionadas: {len(feature_cols)}\")\n",
    "print(\"Features:\", feature_cols)\n",
    "\n",
    "print(f\"\\nTipos de dados:\")\n",
    "print(X.dtypes.value_counts())\n",
    "\n",
    "# Separar colunas numéricas e categóricas\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nColunas numéricas: {len(numeric_cols)}\")\n",
    "print(f\"Colunas categóricas: {len(categorical_cols)}\")\n",
    "\n",
    "# Codificação de variáveis categóricas\n",
    "if categorical_cols:\n",
    "    print(\"Aplicando Label Encoding nas variáveis categóricas...\")\n",
    "    le_dict = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        le_dict[col] = le\n",
    "        print(f\"  {col}: {len(le.classes_)} categorias únicas\")\n",
    "\n",
    "# Verificação da qualidade dos dados\n",
    "print(f\"\\nVerificação de qualidade:\")\n",
    "print(f\"Valores nulos: {X.isnull().sum().sum()}\")\n",
    "\n",
    "numeric_data = X[numeric_cols] if numeric_cols else X\n",
    "if len(numeric_data.columns) > 0:\n",
    "    inf_count = np.isinf(numeric_data.select_dtypes(include=[np.number])).sum().sum()\n",
    "    print(f\"Valores infinitos: {inf_count}\")\n",
    "    X = X.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Divisão treino/teste\n",
    "print(f\"\\nDivisão dos dados (80/20):\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Treino: {X_train.shape[0]:,} amostras\")\n",
    "print(f\"Teste: {X_test.shape[0]:,} amostras\")\n",
    "print(f\"Distribuição treino - Classe 0: {(y_train==0).sum():,}, Classe 1: {(y_train==1).sum():,}\")\n",
    "print(f\"Distribuição teste - Classe 0: {(y_test==0).sum():,}, Classe 1: {(y_test==1).sum():,}\")\n",
    "\n",
    "# Treinamento do modelo Random Forest\n",
    "print(f\"\\nConfigurando modelo Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=1000,\n",
    "    min_samples_leaf=500,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Treinando modelo...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predições\n",
    "print(\"Gerando predições...\")\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nTreinamento concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do modelo\n",
    "\n",
    "print(\"AVALIAÇÃO DO MODELO\")\n",
    "\n",
    "print(f\"\\nAUC-ROC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "print(f\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nMatriz de Confusão:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Importância das features\n",
    "print(f\"\\n\" + \"=\" * 40)\n",
    "print(\"IMPORTÂNCIA DAS FEATURES\")\n",
    "print(\"=\" * 40)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features mais importantes:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportação dos resultados\n",
    "\n",
    "print(\"Exportando artefatos do modelo...\")\n",
    "\n",
    "# Criar estrutura de pastas\n",
    "base_dir = 'dist/classification'\n",
    "artifacts_dir = os.path.join(base_dir, 'artifacts')\n",
    "\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Estrutura de pastas criada: {base_dir}/\")\n",
    "\n",
    "# 1. Salvar modelo e artefatos\n",
    "print(\"\\n1. Salvando modelo e componentes...\")\n",
    "\n",
    "model_artifacts = {\n",
    "    'model': rf_model,\n",
    "    'feature_columns': feature_cols,\n",
    "    'label_encoders': le_dict if 'le_dict' in locals() else {},\n",
    "    'data_corte': data_corte,\n",
    "    'trained_date': datetime.now(),\n",
    "    'model_version': 'RandomForest_v1.0',\n",
    "    'performance_metrics': {\n",
    "        'auc_roc': round(roc_auc_score(y_test, y_pred_proba), 4),\n",
    "        'accuracy': round((y_pred == y_test).mean(), 4),\n",
    "        'precision_class_1': round(classification_report(y_test, y_pred, output_dict=True)['1']['precision'], 4),\n",
    "        'recall_class_1': round(classification_report(y_test, y_pred, output_dict=True)['1']['recall'], 4),\n",
    "        'test_samples': len(y_test)\n",
    "    },\n",
    "    'feature_importance': feature_importance.to_dict('records')\n",
    "}\n",
    "\n",
    "# Salvar modelo\n",
    "model_path = os.path.join(artifacts_dir, 'modelo_recompra_30dias.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print(f\"Modelo salvo em: {model_path}\")\n",
    "\n",
    "# Salvar importância das features\n",
    "feature_importance_path = os.path.join(artifacts_dir, 'feature_importance.csv')\n",
    "feature_importance.to_csv(feature_importance_path, index=False)\n",
    "print(f\"Feature importance salva em: {feature_importance_path}\")\n",
    "\n",
    "# 2. Criar dataset com predições\n",
    "print(\"\\n2. Criando dataset com predições...\")\n",
    "\n",
    "dataset_completo = dataset_final.copy()\n",
    "\n",
    "# Preparar dados para predição\n",
    "X_full = dataset_final[feature_cols].copy()\n",
    "if 'le_dict' in locals():\n",
    "    for col in categorical_cols:\n",
    "        if col in le_dict:\n",
    "            X_full[col] = le_dict[col].transform(X_full[col].astype(str))\n",
    "\n",
    "# Fazer predições para todo o dataset\n",
    "dataset_completo['probabilidade_compra'] = rf_model.predict_proba(X_full)[:, 1]\n",
    "dataset_completo['predicao_compra'] = rf_model.predict(X_full)\n",
    "\n",
    "# Criar categorias de potencial\n",
    "dataset_completo['potencial_recompra'] = pd.cut(\n",
    "    dataset_completo['probabilidade_compra'], \n",
    "    bins=[0, 0.1, 0.3, 0.6, 1.0],\n",
    "    labels=['Baixo', 'Médio', 'Alto', 'Muito Alto']\n",
    ")\n",
    "\n",
    "# Adicionar informações temporais\n",
    "dataset_completo['mes_ultima_compra'] = dataset_completo['data_ultima_compra'].dt.month\n",
    "dataset_completo['ano_ultima_compra'] = dataset_completo['data_ultima_compra'].dt.year\n",
    "dataset_completo['data_predicao'] = datetime.now()\n",
    "dataset_completo['versao_modelo'] = 'RandomForest_v1.0'\n",
    "\n",
    "# Selecionar colunas para exportação\n",
    "colunas_dataset = [\n",
    "    'fk_contact', 'data_ultima_compra', 'target', \n",
    "    'probabilidade_compra', 'predicao_compra', 'potencial_recompra',\n",
    "    'gmv_ultima_compra', 'tickets_ultima_compra',\n",
    "    'dias_desde_ultima_compra', 'total_compras', 'gmv_total', 'gmv_medio',\n",
    "    'mes_ultima_compra', 'ano_ultima_compra',\n",
    "    'origens_unicas', 'destinos_unicos', 'empresas_unicas',\n",
    "    'intervalo_medio_dias', 'regularidade', 'data_predicao', 'versao_modelo'\n",
    "]\n",
    "\n",
    "dataset_final_export = dataset_completo[colunas_dataset].copy()\n",
    "dataset_final_export['potencial_recompra'] = dataset_final_export['potencial_recompra'].astype(str)\n",
    "dataset_final_export['probabilidade_compra'] = dataset_final_export['probabilidade_compra'].round(6)\n",
    "\n",
    "# Salvar CSV\n",
    "csv_path = os.path.join(base_dir, 'dataset_recompra_completo.csv')\n",
    "dataset_final_export.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Dataset preparado: {dataset_final_export.shape[0]:,} registros\")\n",
    "print(f\"Dataset salvo em: {csv_path}\")\n",
    "\n",
    "# 3. Criar metadados\n",
    "print(\"\\n3. Criando arquivo de metadados...\")\n",
    "\n",
    "potencial_stats = dataset_final_export['potencial_recompra'].value_counts().to_dict()\n",
    "\n",
    "metadata = {\n",
    "    'projeto': 'Modelo de Classificação - Predição de Recompra em 30 dias',\n",
    "    'versao_modelo': 'RandomForest_v1.0',\n",
    "    'data_criacao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'data_corte': data_corte.strftime('%Y-%m-%d'),\n",
    "    'dataset': {\n",
    "        'total_clientes': len(dataset_final_export),\n",
    "        'total_features': len(feature_cols)\n",
    "    },\n",
    "    'performance': {\n",
    "        'auc_roc': round(roc_auc_score(y_test, y_pred_proba), 4),\n",
    "        'accuracy': round((y_pred == y_test).mean(), 4),\n",
    "        'precision_class_1': round(classification_report(y_test, y_pred, output_dict=True)['1']['precision'], 4),\n",
    "        'recall_class_1': round(classification_report(y_test, y_pred, output_dict=True)['1']['recall'], 4)\n",
    "    },\n",
    "    'distribuicao_potencial': {\n",
    "        'Baixo': int(potencial_stats.get('Baixo', 0)),\n",
    "        'Médio': int(potencial_stats.get('Médio', 0)),\n",
    "        'Alto': int(potencial_stats.get('Alto', 0)),\n",
    "        'Muito Alto': int(potencial_stats.get('Muito Alto', 0))\n",
    "    },\n",
    "    'features_importantes': [\n",
    "        {'feature': row['feature'], 'importance': round(row['importance'], 4)}\n",
    "        for _, row in feature_importance.head(10).iterrows()\n",
    "    ],\n",
    "    'arquivos_gerados': {\n",
    "        'modelo_completo': 'artifacts/modelo_recompra_30dias.pkl',\n",
    "        'feature_importance': 'artifacts/feature_importance.csv',\n",
    "        'dataset_csv': 'dataset_recompra_completo.csv',\n",
    "        'metadados': 'metadata.json'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvar metadados\n",
    "metadata_path = os.path.join(base_dir, 'metadata.json')\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Metadados salvos em: {metadata_path}\")\n",
    "\n",
    "print(f\"\\nDistribuição por potencial de recompra:\")\n",
    "for categoria in ['Muito Alto', 'Alto', 'Médio', 'Baixo']:\n",
    "    count = potencial_stats.get(categoria, 0)\n",
    "    pct = count/len(dataset_final_export)*100 if len(dataset_final_export) > 0 else 0\n",
    "    print(f\"  {categoria:12}: {count:6,} clientes ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nPerformance do Modelo:\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"Recall (classe 1): {classification_report(y_test, y_pred, output_dict=True)['1']['recall']:.2%}\")\n",
    "\n",
    "print(f\"\\nTop 5 features mais importantes:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(5).iterrows(), 1):\n",
    "    print(f\"  {i}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nExportação concluída com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
